{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2709986d-953a-4000-9c54-aac504e6b9c0",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9001e9-fd87-4a95-b8a3-0fe8d9f3a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705960d0-23af-45e0-a5e6-9b04a7e7713e",
   "metadata": {},
   "source": [
    "## **Basic Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4641d9-3242-4e53-892f-d81e7a218c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x.set_index('id', inplace=True)\n",
    "    x.drop(labels=['year', 'fw_start','fw_end','country','c_abrv'], axis=1, inplace=True) # Remove 'country', 'c_abrc' ?\n",
    "    categorical_cols = x.select_dtypes(include=['object']).columns\n",
    "    x[categorical_cols] = x[categorical_cols].astype('category')\n",
    "    # x = pd.get_dummies(x, columns = categorical_cols)\n",
    "    return x\n",
    "\n",
    "# True Setup\n",
    "X = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y = pd.read_csv('Y_train.csv')\n",
    "X = preprocess(X)\n",
    "X_test = preprocess(X_test)\n",
    "\n",
    "y.set_index('id', inplace=True)\n",
    "y.loc[y['label']==-1] = 0\n",
    "y = y['label']\n",
    "\n",
    "# Handle Missing Values\n",
    "naIndices = X.index[X.isna().any(axis=1)]\n",
    "X = X.drop(naIndices)\n",
    "y = y.drop(naIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40afd05-827e-48b1-bf7d-72b570b729b1",
   "metadata": {},
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ffeb98-3dec-4d10-9fe2-aaa6c8d5307e",
   "metadata": {},
   "source": [
    "Top 500 Features from Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e0c7a-3147-46af-a24b-82995ff6435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RF\n",
    "randomForestModel = RandomForestClassifier(max_features=int(len(X.columns)**(1/2)), random_state=0, oob_score=True)\n",
    "randomForestModel.fit(X, y)\n",
    "\n",
    "# Extract & Sort Features by Importance\n",
    "featureImportance = randomForestModel.feature_importances_\n",
    "\n",
    "i = featureImportance.argsort()\n",
    "features = X.columns[i]\n",
    "importance = featureImportance[i]\n",
    "\n",
    "# Visualize Top 10\n",
    "features = features[-10:]\n",
    "importance = importance[-10:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(features)), importance,color='skyblue')\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel('Feature Influence')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 10 Variable Influence for RF model')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "featureImportancesRF = {'Feature': features, 'Feature Influence': importance}\n",
    "pd.DataFrame(featureImportancesRF).sort_values('Feature Influence', ascending=False).to_excel('important_features_rf.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a4e39-3476-4563-aa50-7f0807122045",
   "metadata": {},
   "source": [
    "Top 0.5 Importance Variables from Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6de08-1fba-4312-9479-6375be55853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GB\n",
    "defaultGBModel.fit(X, y)\n",
    "importance_scores = defaultGBModel.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a dataframe to store feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance_scores})\n",
    "\n",
    "# Sort the dataframe by importance score in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# top_10_features = feature_importance_df.head(10)\n",
    "top_features = []\n",
    "importance_sum = 0\n",
    "\n",
    "# Iterate through the sorted feature importance list\n",
    "for index, row in feature_importance_df.iterrows():\n",
    "    # Add the feature and its importance score to the top features list\n",
    "    top_features.append({'Feature': row['Feature'], 'Importance': row['Importance']})\n",
    "    # Accumulate the importance score\n",
    "    importance_sum += row['Importance']\n",
    "    # Check if the sum exceeds 0.5\n",
    "    # if importance_sum > 0.5:\n",
    "    #     break\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "top_features_df = pd.DataFrame(top_features)\n",
    "top_features_df = top_features_df.iloc[:10]\n",
    "\n",
    "# Plot feature importance for the top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features_df['Feature'], top_features_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 10 Feature Importance from XGBoost')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "top_features_df.to_excel('top_features.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03ef23-aaaf-41ac-ad6c-819ed3930798",
   "metadata": {},
   "source": [
    "## **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85786e6d-f69c-4376-ac95-2403a84283cf",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713319ae-01b5-4e13-a0bb-01079257fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "param_grid = {\n",
    "     'max_depth': [5,9,12],                # Maximum depth of the trees\n",
    "     'learning_rate': [0.01, 0.1, 0.3],    # Learning rate\n",
    "     'subsample': [0.5, 0.7],              # Subsample ratio of the training instances\n",
    "     'colsample_bytree': [0.5, 0.7, 0.9],  # Subsample ratio of columns when constructing each tree\n",
    "     'gamma': [0, 0.1, 1]                  # Minimum loss reduction required to make a further partition on a leaf node\n",
    " }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=boostingModel, param_grid=param_grid, cv=5, scoring='neg_log_loss')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Default Variables\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Use softmax for multi-class classification\n",
    "    'num_class': 5,  # Number of classes\n",
    "    'eval_metric': 'mlogloss',  # Use multi-class log loss as evaluation metric\n",
    "    'eta': 0.01,  # Learning rate\n",
    "    'max_depth': 8,  # Maximum depth of a tree\n",
    "    'subsample': 0.8,  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree\n",
    "    'gamma': 0.01,  # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'enable_categorical': True,\n",
    "    'alpha': 3.5,\n",
    "    'lambda': 3.5,\n",
    "    'min_child_weight': 1,\n",
    "    'n_jobs':4,\n",
    "    'n_estimators':2000,\n",
    "    'device':'cuda'\n",
    "}\n",
    "boostingModel_allVar = XGBClassifier(**params)\n",
    "scores_allVar = cross_val_score(boostingModel_allVar, X, y, cv=4, scoring=\"neg_log_loss\")\n",
    "print(np.mean(scores_allVar))\n",
    "\n",
    "# Top 500 Variables from Random Forest\n",
    "top_features = pd.read_excel('important_features_rf.xlsx').iloc[:500]\n",
    "top_features = top_features['Feature'].values\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Use softmax for multi-class classification\n",
    "    'num_class': 5,  # Number of classes\n",
    "    'eval_metric': 'mlogloss',  # Use multi-class log loss as evaluation metric\n",
    "    'eta': 0.01,  # Learning rate\n",
    "    'max_depth': 8,  # Maximum depth of a tree\n",
    "    'subsample': 0.8,  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree\n",
    "    'gamma': 0.01,  # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    # 'enable_categorical': True,\n",
    "    'alpha': 3, # L1 regularization term\n",
    "    'lambda': 3, # L2 regularization term\n",
    "    'min_child_weight': 4, \n",
    "    'n_estimators':2000, # Iterations/Trees\n",
    "    'device':'cuda'\n",
    "\n",
    "}\n",
    "boostingModel_RFVar = XGBClassifier(**params)\n",
    "scores_RFVar = cross_val_score(boostingModel_RFVar, X, y, cv=4, scoring=\"neg_log_loss\")\n",
    "print(np.mean(scores_RFVar))\n",
    "\n",
    "# Top 0.5 Importance Variables from Gradient Boosting\n",
    "top_features = pd.read_excel('top_features.xlsx')\n",
    "top_features = top_features['Feature'].values\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Use softmax for multi-class classification\n",
    "    'num_class': 5,  # Number of classes\n",
    "    'eval_metric': 'mlogloss',  # Use multi-class log loss as evaluation metric\n",
    "    'eta': 0.01,  # Learning rate\n",
    "    'max_depth': 8,  # Maximum depth of a tree\n",
    "    'subsample': 0.8,  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree\n",
    "    'gamma': 0.01,  # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'enable_categorical': True,\n",
    "    'alpha': 3.5,\n",
    "    'lambda': 3.5,\n",
    "    'min_child_weight': 1,\n",
    "    'n_jobs':4,\n",
    "    'n_estimators':2000,\n",
    "    'device':'cuda'\n",
    "\n",
    "}\n",
    "boostingModel_GBVar = XGBClassifier(**params)\n",
    "scores_GBVar = cross_val_score(boostingModel_GBVar, X, y, cv=4, scoring=\"neg_log_loss\")\n",
    "print(np.mean(scores_GBVar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a341ea-c265-4e0a-bbb4-6988deb6e0a5",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99bbf2-3102-42ca-be8f-451a5fcadf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps including imputation\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', make_pipeline(SimpleImputer(strategy='mean'), StandardScaler()), numerical_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the pipeline including preprocessing and SVM\n",
    "svm_pipeline = make_pipeline(preprocessor, SVC())\n",
    "\n",
    "# Train the SVM model\n",
    "SVMModel = svm_pipeline.fit(X, y)\n",
    "\n",
    "scores_SVM = cross_val_score(SVMModel, X, y, cv=4, scoring=\"neg_log_loss\")\n",
    "print(np.mean(scores_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fdbb6-b5f5-4ba7-ad4f-16346675b21a",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74154e7b-57c4-4734-9179-d1e96f6bec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1000, activation='relu', input_shape=(5,)))\n",
    "# model.add(layers.Dropout(0.2))\n",
    "# model.add(layers.Dense(1000, activation='relu'))\n",
    "# model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.025, momentum=0.8),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X,\n",
    "                    y,\n",
    "                    epochs=250,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(X_test, y))\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb941ac-7bf7-485e-be21-319aa7c9aff2",
   "metadata": {},
   "source": [
    "### Stacking: XGBoost + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4afb68-fd9c-4a57-a593-faacc66313b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest and XGBoost models\n",
    "randomForestModel = RandomForestClassifier(max_features=int(np.sqrt(X_train.shape[1])), random_state=0, oob_score=True)\n",
    "randomForestModel.fit(X_train, y_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Use softmax for multi-class classification\n",
    "    'num_class': 5,  # Number of classes\n",
    "    'eval_metric': 'mlogloss',  # Use multi-class log loss as evaluation metric\n",
    "    'eta': 0.01,  # Learning rate\n",
    "    'max_depth': 8,  # Maximum depth of a tree\n",
    "    'subsample': 0.8,  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree\n",
    "    'gamma': 0.01,  # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'alpha': 3,\n",
    "    'lambda': 3,\n",
    "    'min_child_weight': 4,\n",
    "    'n_jobs': 4,\n",
    "    'n_estimators': 2000\n",
    "}\n",
    "\n",
    "boostingModel = XGBClassifier(**params)\n",
    "boostingModel.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on the validation data\n",
    "randomForestPred = randomForestModel.predict_proba(X_test)\n",
    "xgboostPred = boostingModel.predict_proba(X_test)\n",
    "rf_pred_proba_train = randomForestModel.predict_proba(X_train)\n",
    "xgb_pred_proba_train = boostingModel.predict_proba(X_train)\n",
    "\n",
    "# Combine predictions into a new feature matrix\n",
    "stacked_X_test = np.concatenate((randomForestPred, xgboostPred), axis=1)\n",
    "\n",
    "# Concatenate the predicted probabilities from both models along axis 1 (columns)\n",
    "stacked_X_train = np.concatenate((rf_pred_proba_train, xgb_pred_proba_train), axis=1)\n",
    "\n",
    "# Train a meta-model (Logistic Regression) on the combined predictions\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "meta_model.fit(stacked_X_train, y_train)  # Assuming stacked_X_train is defined\n",
    "\n",
    "# Use the meta-model to make predictions on the test data\n",
    "stacked_pred_proba = meta_model.predict_proba(stacked_X_test)\n",
    "\n",
    "# Define the cross-validation strategy (Stratified K-Folds with k=5)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and calculate log loss scores\n",
    "log_loss_scores = cross_val_score(meta_model, stacked_X_train, y_train, scoring='neg_log_loss', cv=cv)\n",
    "\n",
    "# Print log loss scores\n",
    "print(\"Log Loss Scores:\", log_loss_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda62736-a0ba-4d64-83a0-29943f033860",
   "metadata": {},
   "source": [
    "## **Comparisons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e2cd9-9074-4458-8999-ec4013682584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfScores = cross_val_score(randomForestModel, X, y, cv=4, scoring='neg_log_loss')\n",
    "import matplotlib.pyplot as plt\n",
    "data = [rfScores, scores_default, scores_allVar, scores_RFVar, scores_GBVar, svm_xgboosting, xgboosting_rf]\n",
    "\n",
    "plt.boxplot(data)\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel('Negative Log Loss')\n",
    "plt.title('Log loss of 5 XGBoost models and RF model')\n",
    "\n",
    "# Customize x-axis tick labels\n",
    "plt.xticks(range(1, len(data) + 1), ['RF all var','Default all var', 'Tuned all var', 'Top 500 RF Var', 'Top GB Var', 'SVM + XGB'])\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# rfScores = cross_val_score(randomForestModel, X, y, cv=4, scoring='neg_log_loss')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Provided data\n",
    "svm_xgboosting = [-0.87616632, -0.86292975, -0.86713181, -0.87255519, -0.83238594]\n",
    "xgboosting_rf = [-0.86726267, -0.85745867, -0.86215765, -0.85209438, -0.83243321]\n",
    "svm = [0.88714105, 0.88489864, 0.87195326, 0.8755607,  0.89217313]\n",
    "\n",
    "data = [rfScores, scores_default, scores_allVar, scores_RFVar, scores_GBVar, svm_xgboosting, xgboosting_rf]\n",
    "\n",
    "plt.boxplot(data)\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel('Negative Log Loss')\n",
    "plt.title('Log loss of 5 XGBoost models and RF model')\n",
    "\n",
    "# Customize x-axis tick labels\n",
    "plt.xticks(range(1, len(data) + 1), ['RF all var','Default all var', 'Tuned all var', 'Top 500 RF Var', 'Top GB Var', 'SVM + XGB'])\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
